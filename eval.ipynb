{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\NLP_project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.data.processing import tokenize_dataset\n",
    "import transformers\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments,DataCollatorForSeq2Seq, AutoTokenizer\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\NLP_project\\venv\\lib\\site-packages\\datasets\\load.py:1491: FutureWarning: The repository for Samsung/samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Samsung/samsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_dataset(\"Samsung/samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\NLP_project\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_LOCAL = \"./.output/t5-summarizer/results\"#\"google-t5/t5-small\"\n",
    "LOCAL = True\n",
    "DEVICE = \"cuda\"\n",
    "model1 = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"lucapl/t5-summarizer-samsum\",local_files_only=False, device_map=DEVICE)\n",
    "#tokenizer = transformers.AutoTokenizer.from_pretrained(model_local,local_files_only=LOCAL, device_map=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google-t5/t5-small\"\n",
    "model2 = transformers.AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME,local_files_only=False, device_map=DEVICE)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME,local_files_only=False, device_map=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15551/15551 [00:00<00:00, 29793.46 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14732/14732 [00:03<00:00, 4440.02 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 819/819 [00:00<00:00, 4178.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 818/818 [00:00<00:00, 4130.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenize_dataset(data, tokenizer, True,data_prefix=\"summarize: \")\n",
    "tokenized_dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_16612\\2953629268.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = datasets.load_metric(\"rouge\")\n",
      "z:\\NLP_project\\venv\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\".output/t5-summarizer\",\n",
    "#     learning_rate=1e-3,\n",
    "#     per_device_train_batch_size=64,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "# \tremove_unused_columns=False \n",
    "# )\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer,model,\n",
    "# \treturn_tensors=\"pt\",\n",
    "# \tlabel_pad_token_id=-100,\n",
    "# \tpad_to_multiple_of=8)\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# eval_results = trainer.predict(tokenized_dataset[\"test\"].select(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model1.generate(tokenized_dataset[\"validation\"].select(range(25))[\"input_ids\"].to(\"cuda\"),max_new_tokens=100)\n",
    "out2 = model2.generate(tokenized_dataset[\"validation\"].select(range(25))[\"input_ids\"].to(\"cuda\"),max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue: Laura: I need a new printer :/\n",
      "Laura: thinking about this one\n",
      "Laura: <file_other>\n",
      "Jamie: you're sure you need a new one?\n",
      "Jamie: I mean you can buy a second hand one\n",
      "Laura: could be\n",
      "\n",
      "t5 finetuned: Laura needs a new printer. Jamie is sure she needs a new printer.\n",
      "\n",
      "t5: Laura: you're sure you need a new one Laura: file_other> Jamie: you're sure you need a new one?\n",
      "\n",
      "expected: Laura is going to buy a printer.\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "print(\"Dialogue:\",data[\"validation\"][n][\"dialogue\"])\n",
    "print()\n",
    "print(\"t5 finetuned:\",tokenizer.decode(out1[n],skip_special_tokens=True))\n",
    "print()\n",
    "print(\"t5:\",tokenizer.decode(out2[n],skip_special_tokens=True))\n",
    "print()\n",
    "print(\"expected:\",data[\"validation\"][n][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.push_to_hub(\"t5-summarizer-samsum\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
