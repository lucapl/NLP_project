{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.data.processing import tokenize_dataset\n",
    "import transformers\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments,DataCollatorForSeq2Seq, AutoTokenizer\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\NLP_project\\venv\\lib\\site-packages\\datasets\\load.py:1491: FutureWarning: The repository for Samsung/samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Samsung/samsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_dataset(\"Samsung/samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\NLP_project\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_LOCAL = \"./.output/t5-summarizer/results\"#\"google-t5/t5-small\"\n",
    "LOCAL = True\n",
    "DEVICE = \"cuda\"\n",
    "model1 = transformers.AutoModelForSeq2SeqLM.from_pretrained(MODEL_LOCAL,local_files_only=LOCAL, device_map=DEVICE)\n",
    "#tokenizer = transformers.AutoTokenizer.from_pretrained(model_local,local_files_only=LOCAL, device_map=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google-t5/t5-small\"\n",
    "model2 = transformers.AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME,local_files_only=False, device_map=DEVICE)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME,local_files_only=False, device_map=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 255\n",
      "Max target length: 45\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenize_dataset(data, tokenizer, True)\n",
    "tokenized_dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_8920\\2953629268.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = datasets.load_metric(\"rouge\")\n",
      "z:\\NLP_project\\venv\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\NLP_project\\venv\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 39.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\".output/t5-summarizer\",\n",
    "#     learning_rate=1e-3,\n",
    "#     per_device_train_batch_size=64,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "# \tremove_unused_columns=False \n",
    "# )\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer,model,\n",
    "# \treturn_tensors=\"pt\",\n",
    "# \tlabel_pad_token_id=-100,\n",
    "# \tpad_to_multiple_of=8)\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# eval_results = trainer.predict(tokenized_dataset[\"test\"].select(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model1.generate(tokenized_dataset[\"validation\"].select(range(20))[\"input_ids\"].to(\"cuda\"),max_new_tokens=100)\n",
    "out2 = model2.generate(tokenized_dataset[\"validation\"].select(range(20))[\"input_ids\"].to(\"cuda\"),max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue: Keith: Meg, pls buy some milk and cereals, I see now we've run out of them\n",
      "Megan: hm, sure, I can do that\n",
      "Megan: but did you check in the drawer next to the fridge?\n",
      "Keith: nope, let me have a look\n",
      "Keith: ok, false alarm, we have cereal and milk :D\n",
      "Megan: <file_gif>\n",
      "t5 finetuned: Keith and Megan are buying milk and cereals. They have run out of them.\n",
      "t5: Keith: ok, false alarm, we have cereal and milk. he says we have cereal and milk.\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "print(\"Dialogue:\",data[\"validation\"][n][\"dialogue\"])\n",
    "print(\"t5 finetuned:\",tokenizer.decode(out1[n],skip_special_tokens=True))\n",
    "print(\"t5:\",tokenizer.decode(out2[n],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gina: ok, what do u need? Wanda: ok, what do u need? Gina: ok, what do u need? Wanda: u know, but u can ask.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
